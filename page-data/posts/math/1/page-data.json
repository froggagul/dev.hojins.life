{
    "componentChunkName": "component---src-templates-blog-post-tsx",
    "path": "/posts/math/1/",
    "result": {"pageContext":{"html":"<h2>Forward Diffusion Process</h2>\n<p>$$\n\\text{Goal: Gradually add Gausian noise and then reverse}<br>\nx_0 \\text{\\textasciitilde} q(x)\\ \\ \\ \\ \\text{(original data)}\n$$</p>\n<p>$$\n{\\beta<em>t\\in(0,1)}^t</em>{t=1} \\text{ is hyperparameter}\nq(x<em>t|x</em>{t-1})=\\mathcal N(x<em>t;\\sqrt{1-\\beta</em>t}x<em>{t-1}, \\beta</em>tI)\nq(x<em>T|x</em>0)=\\prod<em>{t=1}^Tq(x</em>t|x_{t-1})\n$$</p>\n<ul>\n<li>q is markov chain</li>\n<li>hyperparameter를 잘 조작하면 $lim<em>{x\\rightarrow\\infin}T</em>x$ follows isotropic gaussian distribution</li>\n</ul>\n<h3>reparameterization trick</h3>\n<p>$$\nq(x<em>t|x</em>{t-1})=\\mathcal N(x<em>t;\\sqrt{1-\\beta</em>t}x<em>{t-1}, \\beta</em>tI)\n\\alpha<em>t=1-\\beta</em>t,\\  \\bar\\alpha<em>t=\\prod</em>{i=1}^t\\alpha<em>i\n;\nx</em>t=\\sqrt{\\alpha<em>t}x</em>{t-1}+\\sqrt{1-\\alpha<em>t}z</em>{t-1} ;;;(z<em>{t-1}\\text{\\textasciitilde} \\mathcal N(0, 1))\n= \\sqrt{\\alpha</em>t\\alpha<em>{t-1}}x</em>{t-2} + \\sqrt{1-\\alpha<em>t}z</em>{t-1} + \\sqrt{\\alpha<em>t(1-\\alpha</em>{t-1})}z<em>{t-2}\n= \\sqrt{\\alpha</em>t\\alpha<em>{t-1}}x</em>{t-2} + \\sqrt{1-\\alpha<em>t\\alpha</em>{t-1}}\\bar z<em>{t-2};(\\text{merge two Gaussian})\n= \\sqrt{\\bar\\alpha</em>t}x<em>0+\\sqrt{1-\\bar\\alpha</em>t}z\n$$\n$$\nq(x<em>t|x</em>0)=\\mathcal N(x<em>t;\\bar\\alpha</em>t x<em>0, (1 - \\bar\\alpha</em>t)I)\n$$</p>\n<h2>Reverse Diffusion Process</h2>\n<ul>\n<li>역방향을 재생성할 수 있다면 좋겠지만, 현실적으로 비효율적이다. 전체 dataset을 관찰해야하기 때문이다.</li>\n</ul>\n<p>$$\nP<em>\\theta(x</em>{0:T})=p(x<em>T)\\prod</em>{t=1}^T p<em>\\theta(x</em>{t-1}|x<em>t)\np</em>\\theta(x<em>{t-1}|x</em>t)=\\mathcal N(x<em>{t-1};\\mu</em>\\theta(x<em>t, t), \\Sigma</em>\\theta(x_t, t))\n$$</p>\n<ul>\n<li>각 gaussian process의 mean, variance를 parametrization하여 예측한다.</li>\n<li>true reverse process를 알 수 없으므로 condition에 $x_0$를 추가한다. 이는 계산 가능하다.</li>\n</ul>\n<p>$$\nq(x<em>{t-1}|x</em>t)\nq(x<em>t|x</em>{t-1})\n$$\n$$\nq(x<em>{t-1}|x</em>t, x<em>0)=\\mathcal N(x</em>{t-1};\\tilde{\\mu}(x<em>t, x</em>0), \\tilde{\\beta<em>t}I)\nq(x</em>{t-1}|x<em>t, x</em>0) = q(x<em>t|x</em>{t-1}, x<em>0)\\frac{q(x</em>{t-1}| x<em>0)}{q(x</em>t| x<em>0)}\n\\propto exp(-\\frac{1}{2}(\\frac{(x</em>t-\\sqrt{\\alpha<em>t}x</em>{t-1})^2}{\\beta<em>t} + \\frac{(x</em>{t-1}-\\sqrt{\\alpha<em>{t-1}}x</em>0)^2}{1-\\bar\\alpha<em>{t-1}}) - \\frac{(x</em>t-\\sqrt{\\bar\\alpha<em>t}x</em>0)^2}{1-\\bar\\alpha<em>t})\n= exp(\n-\\frac{1}{2}\n(\n(\n\\frac{\\alpha</em>t}{\\beta<em>t} +\n\\frac{1}{1-\\bar\\alpha</em>{t-1}}\n) x<em>{t-1}^2 -\n2 (\n\\frac{\n2\\sqrt\\alpha</em>t\n}{\n\\beta<em>t\n} x</em>t +\n\\frac{\n2\\sqrt{\n\\bar\\alpha<em>t\n}\n}{\n1-\\bar\\alpha</em>{t-1}\n} x<em>0\n) x</em>{t-1} +\nC(x<em>t, x</em>0)\n)\n)\n$$\n$$\n\\tilde{\\beta<em>t}\n= \\frac{\n1-\\bar{\\alpha}</em>{t-1}\n} {\n1-\\bar{\\alpha<em>{t}}\n} \\beta</em>t\n\\tilde{\\mu}(x<em>t, x</em>0) =\n\\frac{\n1\n} {\n\\sqrt{\\alpha<em>t}\n} (\nx</em>t -\n\\frac{\n\\beta<em>t\n} {\n\\sqrt{1-\\bar\\alpha</em>t}\n} z_t\n)\n$$</p>\n<ul>\n<li>\n<p>we can use VLB as our loss.</p>\n<h3>ELBO</h3>\n<p>$$\nL\n= - \\int q(x<em>0)\\log{p(x</em>0)} dx<em>0\n= - E</em>{q(x<em>0)}[\\log{p</em>\\theta(x<em>0)}]\n= - E</em>{q(x<em>0)}[\\log{\\frac{\np</em>\\theta(x<em>{0:T})\n}{\np</em>\\theta(x<em>{1:T}|x</em>0)\n}}]\n= - E<em>{q(x</em>0)}[\\log{\n\\frac{\np<em>\\theta(x</em>{0:T})\n}{\np<em>\\theta(x</em>{1:T}|x<em>0)\n}\n\\frac{\nq</em>\\theta(x<em>{1:T}|x</em>0)\n}{\nq<em>\\theta(x</em>{1:T}|x_0)\n}\n}]\n\\leq - E<em>{q(x</em>0)}[\\log{\n\\frac{\np<em>\\theta(x</em>{0:T})\n}{\nq<em>\\theta(x</em>{1:T}|x_0)\n}\n}]\n]] = L_{VLB}\n$$</p>\n</li>\n</ul>\n<p>$$\n\\mu<em>\\theta(x</em>t, t) = \\frac{1}{\\sqrt{\\alpha<em>t}}(x</em>t-\\frac{\\beta<em>t}{\\sqrt{1-\\bar\\alpha</em>t}}z<em>\\theta(x</em>t, t))\n$$</p>","title":"So, what is DDPM?","date":"Sat Dec 04 2021","series":{"series":"math","ep":1,"max":1},"next":null}},
    "staticQueryHashes": ["2492723766"]}