<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[Hojin's Note | rss]]></title><description><![CDATA[I'm writing my own feelings on this blog]]></description><link>http://github.com/dylang/node-rss</link><generator>GatsbyJS</generator><lastBuildDate>Sat, 04 Dec 2021 16:47:37 GMT</lastBuildDate><item><title><![CDATA[No title]]></title><description><![CDATA[Forward Diffusion Process $$
\text{Goal: Gradually add Gausian noise and then reverse} x_0 \text{\textasciitilde} q(x)\ \ \ \ \text…]]></description><link>https://www.hojins.life/posts/math/1/</link><guid isPermaLink="false">https://www.hojins.life/posts/math/1/</guid><pubDate>Sat, 04 Dec 2021 00:00:00 GMT</pubDate><content:encoded>&lt;h2&gt;Forward Diffusion Process&lt;/h2&gt;
&lt;p&gt;$$
\text{Goal: Gradually add Gausian noise and then reverse}&lt;br&gt;
x_0 \text{\textasciitilde} q(x)\ \ \ \ \text{(original data)}
$$&lt;/p&gt;
&lt;p&gt;$$
{\beta&lt;em&gt;t\in(0,1)}^t&lt;/em&gt;{t=1} \text{ is hyperparameter}
q(x&lt;em&gt;t|x&lt;/em&gt;{t-1})=\mathcal N(x&lt;em&gt;t;\sqrt{1-\beta&lt;/em&gt;t}x&lt;em&gt;{t-1}, \beta&lt;/em&gt;tI)
q(x&lt;em&gt;T|x&lt;/em&gt;0)=\prod&lt;em&gt;{t=1}^Tq(x&lt;/em&gt;t|x_{t-1})
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;q is markov chain&lt;/li&gt;
&lt;li&gt;hyperparameter를 잘 조작하면 $lim&lt;em&gt;{x\rightarrow\infin}T&lt;/em&gt;x$ follows isotropic gaussian distribution&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;reparameterization trick&lt;/h3&gt;
&lt;p&gt;$$
q(x&lt;em&gt;t|x&lt;/em&gt;{t-1})=\mathcal N(x&lt;em&gt;t;\sqrt{1-\beta&lt;/em&gt;t}x&lt;em&gt;{t-1}, \beta&lt;/em&gt;tI)
\alpha&lt;em&gt;t=1-\beta&lt;/em&gt;t,\  \bar\alpha&lt;em&gt;t=\prod&lt;/em&gt;{i=1}^t\alpha&lt;em&gt;i
;
x&lt;/em&gt;t=\sqrt{\alpha&lt;em&gt;t}x&lt;/em&gt;{t-1}+\sqrt{1-\alpha&lt;em&gt;t}z&lt;/em&gt;{t-1} ;;;(z&lt;em&gt;{t-1}\text{\textasciitilde} \mathcal N(0, 1))
= \sqrt{\alpha&lt;/em&gt;t\alpha&lt;em&gt;{t-1}}x&lt;/em&gt;{t-2} + \sqrt{1-\alpha&lt;em&gt;t}z&lt;/em&gt;{t-1} + \sqrt{\alpha&lt;em&gt;t(1-\alpha&lt;/em&gt;{t-1})}z&lt;em&gt;{t-2}
= \sqrt{\alpha&lt;/em&gt;t\alpha&lt;em&gt;{t-1}}x&lt;/em&gt;{t-2} + \sqrt{1-\alpha&lt;em&gt;t\alpha&lt;/em&gt;{t-1}}\bar z&lt;em&gt;{t-2};(\text{merge two Gaussian})
= \sqrt{\bar\alpha&lt;/em&gt;t}x&lt;em&gt;0+\sqrt{1-\bar\alpha&lt;/em&gt;t}z
$$
$$
q(x&lt;em&gt;t|x&lt;/em&gt;0)=\mathcal N(x&lt;em&gt;t;\bar\alpha&lt;/em&gt;t x&lt;em&gt;0, (1 - \bar\alpha&lt;/em&gt;t)I)
$$&lt;/p&gt;
&lt;h2&gt;Reverse Diffusion Process&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;역방향을 재생성할 수 있다면 좋겠지만, 현실적으로 비효율적이다. 전체 dataset을 관찰해야하기 때문이다.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
P&lt;em&gt;\theta(x&lt;/em&gt;{0:T})=p(x&lt;em&gt;T)\prod&lt;/em&gt;{t=1}^T p&lt;em&gt;\theta(x&lt;/em&gt;{t-1}|x&lt;em&gt;t)
p&lt;/em&gt;\theta(x&lt;em&gt;{t-1}|x&lt;/em&gt;t)=\mathcal N(x&lt;em&gt;{t-1};\mu&lt;/em&gt;\theta(x&lt;em&gt;t, t), \Sigma&lt;/em&gt;\theta(x_t, t))
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;각 gaussian process의 mean, variance를 parametrization하여 예측한다.&lt;/li&gt;
&lt;li&gt;true reverse process를 알 수 없으므로 condition에 $x_0$를 추가한다. 이는 계산 가능하다.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
q(x&lt;em&gt;{t-1}|x&lt;/em&gt;t)
q(x&lt;em&gt;t|x&lt;/em&gt;{t-1})
$$
$$
q(x&lt;em&gt;{t-1}|x&lt;/em&gt;t, x&lt;em&gt;0)=\mathcal N(x&lt;/em&gt;{t-1};\tilde{\mu}(x&lt;em&gt;t, x&lt;/em&gt;0), \tilde{\beta&lt;em&gt;t}I)
q(x&lt;/em&gt;{t-1}|x&lt;em&gt;t, x&lt;/em&gt;0) = q(x&lt;em&gt;t|x&lt;/em&gt;{t-1}, x&lt;em&gt;0)\frac{q(x&lt;/em&gt;{t-1}| x&lt;em&gt;0)}{q(x&lt;/em&gt;t| x&lt;em&gt;0)}
\propto exp(-\frac{1}{2}(\frac{(x&lt;/em&gt;t-\sqrt{\alpha&lt;em&gt;t}x&lt;/em&gt;{t-1})^2}{\beta&lt;em&gt;t} + \frac{(x&lt;/em&gt;{t-1}-\sqrt{\alpha&lt;em&gt;{t-1}}x&lt;/em&gt;0)^2}{1-\bar\alpha&lt;em&gt;{t-1}}) - \frac{(x&lt;/em&gt;t-\sqrt{\bar\alpha&lt;em&gt;t}x&lt;/em&gt;0)^2}{1-\bar\alpha&lt;em&gt;t})
= exp(
-\frac{1}{2}
(
(
\frac{\alpha&lt;/em&gt;t}{\beta&lt;em&gt;t} +
\frac{1}{1-\bar\alpha&lt;/em&gt;{t-1}}
) x&lt;em&gt;{t-1}^2 -
2 (
\frac{
2\sqrt\alpha&lt;/em&gt;t
}{
\beta&lt;em&gt;t
} x&lt;/em&gt;t +
\frac{
2\sqrt{
\bar\alpha&lt;em&gt;t
}
}{
1-\bar\alpha&lt;/em&gt;{t-1}
} x&lt;em&gt;0
) x&lt;/em&gt;{t-1} +
C(x&lt;em&gt;t, x&lt;/em&gt;0)
)
)
$$
$$
\tilde{\beta&lt;em&gt;t}
= \frac{
1-\bar{\alpha}&lt;/em&gt;{t-1}
} {
1-\bar{\alpha&lt;em&gt;{t}}
} \beta&lt;/em&gt;t
\tilde{\mu}(x&lt;em&gt;t, x&lt;/em&gt;0) =
\frac{
1
} {
\sqrt{\alpha&lt;em&gt;t}
} (
x&lt;/em&gt;t -
\frac{
\beta&lt;em&gt;t
} {
\sqrt{1-\bar\alpha&lt;/em&gt;t}
} z_t
)
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;we can use VLB as our loss.&lt;/p&gt;
&lt;h3&gt;ELBO&lt;/h3&gt;
&lt;p&gt;$$
L
= - \int q(x&lt;em&gt;0)\log{p(x&lt;/em&gt;0)} dx&lt;em&gt;0
= - E&lt;/em&gt;{q(x&lt;em&gt;0)}[\log{p&lt;/em&gt;\theta(x&lt;em&gt;0)}]
= - E&lt;/em&gt;{q(x&lt;em&gt;0)}[\log{\frac{
p&lt;/em&gt;\theta(x&lt;em&gt;{0:T})
}{
p&lt;/em&gt;\theta(x&lt;em&gt;{1:T}|x&lt;/em&gt;0)
}}]
= - E&lt;em&gt;{q(x&lt;/em&gt;0)}[\log{
\frac{
p&lt;em&gt;\theta(x&lt;/em&gt;{0:T})
}{
p&lt;em&gt;\theta(x&lt;/em&gt;{1:T}|x&lt;em&gt;0)
}
\frac{
q&lt;/em&gt;\theta(x&lt;em&gt;{1:T}|x&lt;/em&gt;0)
}{
q&lt;em&gt;\theta(x&lt;/em&gt;{1:T}|x_0)
}
}]
\leq - E&lt;em&gt;{q(x&lt;/em&gt;0)}[\log{
\frac{
p&lt;em&gt;\theta(x&lt;/em&gt;{0:T})
}{
q&lt;em&gt;\theta(x&lt;/em&gt;{1:T}|x_0)
}
}]
]] = L_{VLB}
$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$
\mu&lt;em&gt;\theta(x&lt;/em&gt;t, t) = \frac{1}{\sqrt{\alpha&lt;em&gt;t}}(x&lt;/em&gt;t-\frac{\beta&lt;em&gt;t}{\sqrt{1-\bar\alpha&lt;/em&gt;t}}z&lt;em&gt;\theta(x&lt;/em&gt;t, t))
$$&lt;/p&gt;</content:encoded></item></channel></rss>